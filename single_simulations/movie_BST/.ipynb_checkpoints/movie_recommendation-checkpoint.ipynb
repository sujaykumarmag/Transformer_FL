{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xP1qcW5EEKP2"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XOSGGHmoEKP2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLojwrPsEKP3"
   },
   "source": [
    "\n",
    "## Prepare the data\n",
    "\n",
    "### Download and prepare the DataFrames\n",
    "\n",
    "First, let's download the movielens data.\n",
    "\n",
    "The downloaded folder will contain three data files: `users.dat`, `movies.dat`,\n",
    "and `ratings.dat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Read the file as binary data\n",
    "with open(\"ml-1m/movies.dat\", \"rb\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Decode the data while ignoring invalid characters\n",
    "data_str = data.decode('utf-8', errors='ignore')\n",
    "\n",
    "# Create a StringIO object to read the decoded data\n",
    "data_io = StringIO(data_str)\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "movies = pd.read_csv(\n",
    "    data_io,\n",
    "    sep=\"::\",\n",
    "    names=[\"movie_id\", \"title\", \"genres\"],\n",
    "    engine='python'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>3948</td>\n",
       "      <td>Meet the Parents (2000)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>3949</td>\n",
       "      <td>Requiem for a Dream (2000)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>3950</td>\n",
       "      <td>Tigerland (2000)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>3951</td>\n",
       "      <td>Two Family House (2000)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>3952</td>\n",
       "      <td>Contender, The (2000)</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3883 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movie_id                               title  \\\n",
       "0            1                    Toy Story (1995)   \n",
       "1            2                      Jumanji (1995)   \n",
       "2            3             Grumpier Old Men (1995)   \n",
       "3            4            Waiting to Exhale (1995)   \n",
       "4            5  Father of the Bride Part II (1995)   \n",
       "...        ...                                 ...   \n",
       "3878      3948             Meet the Parents (2000)   \n",
       "3879      3949          Requiem for a Dream (2000)   \n",
       "3880      3950                    Tigerland (2000)   \n",
       "3881      3951             Two Family House (2000)   \n",
       "3882      3952               Contender, The (2000)   \n",
       "\n",
       "                            genres  \n",
       "0      Animation|Children's|Comedy  \n",
       "1     Adventure|Children's|Fantasy  \n",
       "2                   Comedy|Romance  \n",
       "3                     Comedy|Drama  \n",
       "4                           Comedy  \n",
       "...                            ...  \n",
       "3878                        Comedy  \n",
       "3879                         Drama  \n",
       "3880                         Drama  \n",
       "3881                         Drama  \n",
       "3882                Drama|Thriller  \n",
       "\n",
       "[3883 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqMDhWkxEKP3"
   },
   "source": [
    "Then, we load the data into pandas DataFrames with their proper column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HjiUZY4lEKP3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l2/_mks1sd56yjdxq9514qt1fnh0000gn/T/ipykernel_10501/3511900228.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  users = pd.read_csv(\n",
      "/var/folders/l2/_mks1sd56yjdxq9514qt1fnh0000gn/T/ipykernel_10501/3511900228.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  ratings = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "users = pd.read_csv(\n",
    "    \"ml-1m/users.dat\",\n",
    "    sep=\"::\",\n",
    "    names=[\"user_id\", \"sex\", \"age_group\", \"occupation\", \"zip_code\"],\n",
    ")\n",
    "\n",
    "ratings = pd.read_csv(\n",
    "    \"ml-1m/ratings.dat\",\n",
    "    sep=\"::\",\n",
    "    names=[\"user_id\", \"movie_id\", \"rating\", \"unix_timestamp\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI_niMtlEKP4"
   },
   "source": [
    "Here, we do some simple data processing to fix the data types of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Lh8LiA8jEKP4"
   },
   "outputs": [],
   "source": [
    "users[\"user_id\"] = users[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
    "users[\"age_group\"] = users[\"age_group\"].apply(lambda x: f\"group_{x}\")\n",
    "users[\"occupation\"] = users[\"occupation\"].apply(lambda x: f\"occupation_{x}\")\n",
    "\n",
    "movies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
    "\n",
    "ratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
    "ratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
    "ratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_hU_Lk4EKP4"
   },
   "source": [
    "Each movie has multiple genres. We split them into separate columns in the `movies`\n",
    "DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p_JI81ggEKP4"
   },
   "outputs": [],
   "source": [
    "genres = [\n",
    "    \"Action\",\n",
    "    \"Adventure\",\n",
    "    \"Animation\",\n",
    "    \"Children's\",\n",
    "    \"Comedy\",\n",
    "    \"Crime\",\n",
    "    \"Documentary\",\n",
    "    \"Drama\",\n",
    "    \"Fantasy\",\n",
    "    \"Film-Noir\",\n",
    "    \"Horror\",\n",
    "    \"Musical\",\n",
    "    \"Mystery\",\n",
    "    \"Romance\",\n",
    "    \"Sci-Fi\",\n",
    "    \"Thriller\",\n",
    "    \"War\",\n",
    "    \"Western\",\n",
    "]\n",
    "\n",
    "for genre in genres:\n",
    "    movies[genre] = movies[\"genres\"].apply(\n",
    "        lambda values: int(genre in values.split(\"|\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeQ1p_NkEKP4"
   },
   "source": [
    "### Transform the movie ratings data into sequences\n",
    "\n",
    "First, let's sort the the ratings data using the `unix_timestamp`, and then group the\n",
    "`movie_id` values and the `rating` values by `user_id`.\n",
    "\n",
    "The output DataFrame will have a record for each `user_id`, with two ordered lists\n",
    "(sorted by rating datetime): the movies they have rated, and their ratings of these movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OVK63sRNEKP4"
   },
   "outputs": [],
   "source": [
    "ratings_group = ratings.sort_values(by=[\"unix_timestamp\"]).groupby(\"user_id\")\n",
    "\n",
    "ratings_data = pd.DataFrame(\n",
    "    data={\n",
    "        \"user_id\": list(ratings_group.groups.keys()),\n",
    "        \"movie_ids\": list(ratings_group.movie_id.apply(list)),\n",
    "        \"ratings\": list(ratings_group.rating.apply(list)),\n",
    "        \"timestamps\": list(ratings_group.unix_timestamp.apply(list)),\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63QWAI03EKP4"
   },
   "source": [
    "Now, let's split the `movie_ids` list into a set of sequences of a fixed length.\n",
    "We do the same for the `ratings`. Set the `sequence_length` variable to change the length\n",
    "of the input sequence to the model. You can also change the `step_size` to control the\n",
    "number of sequences to generate for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pxfN3C6tEKP5"
   },
   "outputs": [],
   "source": [
    "sequence_length = 4\n",
    "step_size = 2\n",
    "\n",
    "\n",
    "def create_sequences(values, window_size, step_size):\n",
    "    sequences = []\n",
    "    start_index = 0\n",
    "    while True:\n",
    "        end_index = start_index + window_size\n",
    "        seq = values[start_index:end_index]\n",
    "        if len(seq) < window_size:\n",
    "            seq = values[-window_size:]\n",
    "            if len(seq) == window_size:\n",
    "                sequences.append(seq)\n",
    "            break\n",
    "        sequences.append(seq)\n",
    "        start_index += step_size\n",
    "    return sequences\n",
    "\n",
    "\n",
    "ratings_data.movie_ids = ratings_data.movie_ids.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "ratings_data.ratings = ratings_data.ratings.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "del ratings_data[\"timestamps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrcYoSHmEKP5"
   },
   "source": [
    "After that, we process the output to have each sequence in a separate records in\n",
    "the DataFrame. In addition, we join the user features with the ratings data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ssr6-wpOEKP5"
   },
   "outputs": [],
   "source": [
    "ratings_data_movies = ratings_data[[\"user_id\", \"movie_ids\"]].explode(\n",
    "    \"movie_ids\", ignore_index=True\n",
    ")\n",
    "ratings_data_rating = ratings_data[[\"ratings\"]].explode(\"ratings\", ignore_index=True)\n",
    "ratings_data_transformed = pd.concat([ratings_data_movies, ratings_data_rating], axis=1)\n",
    "ratings_data_transformed = ratings_data_transformed.join(\n",
    "    users.set_index(\"user_id\"), on=\"user_id\"\n",
    ")\n",
    "ratings_data_transformed.movie_ids = ratings_data_transformed.movie_ids.apply(\n",
    "    lambda x: \",\".join(x)\n",
    ")\n",
    "ratings_data_transformed.ratings = ratings_data_transformed.ratings.apply(\n",
    "    lambda x: \",\".join([str(v) for v in x])\n",
    ")\n",
    "\n",
    "del ratings_data_transformed[\"zip_code\"]\n",
    "\n",
    "ratings_data_transformed.rename(\n",
    "    columns={\"movie_ids\": \"sequence_movie_ids\", \"ratings\": \"sequence_ratings\"},\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbJDIDP_EKP5"
   },
   "source": [
    "With `sequence_length` of 4 and `step_size` of 2, we end up with 498,623 sequences.\n",
    "\n",
    "Finally, we split the data into training and testing splits, with 85% and 15% of\n",
    "the instances, respectively, and store them to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hJOlxYoaEKP5"
   },
   "outputs": [],
   "source": [
    "random_selection = np.random.rand(len(ratings_data_transformed.index)) <= 0.85\n",
    "train_data = ratings_data_transformed[random_selection]\n",
    "test_data = ratings_data_transformed[~random_selection]\n",
    "\n",
    "train_data.to_csv(\"train_data.csv\", index=False, sep=\"|\", header=False)\n",
    "test_data.to_csv(\"test_data.csv\", index=False, sep=\"|\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB3hOJTtEKP5"
   },
   "source": [
    "## Define metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "85PMidfIEKP5"
   },
   "outputs": [],
   "source": [
    "CSV_HEADER = list(ratings_data_transformed.columns)\n",
    "\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"user_id\": list(users.user_id.unique()),\n",
    "    \"movie_id\": list(movies.movie_id.unique()),\n",
    "    \"sex\": list(users.sex.unique()),\n",
    "    \"age_group\": list(users.age_group.unique()),\n",
    "    \"occupation\": list(users.occupation.unique()),\n",
    "}\n",
    "\n",
    "USER_FEATURES = [\"sex\", \"age_group\", \"occupation\"]\n",
    "\n",
    "MOVIE_FEATURES = [\"genres\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb3OK77EEKP5"
   },
   "source": [
    "## Create `tf.data.Dataset` for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TkGkgrb3EKP5"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
    "    def process(features):\n",
    "        movie_ids_string = features[\"sequence_movie_ids\"]\n",
    "        sequence_movie_ids = tf.strings.split(movie_ids_string, \",\").to_tensor()\n",
    "\n",
    "        # The last movie id in the sequence is the target movie.\n",
    "        features[\"target_movie_id\"] = sequence_movie_ids[:, -1]\n",
    "        features[\"sequence_movie_ids\"] = sequence_movie_ids[:, :-1]\n",
    "\n",
    "        ratings_string = features[\"sequence_ratings\"]\n",
    "        sequence_ratings = tf.strings.to_number(\n",
    "            tf.strings.split(ratings_string, \",\"), tf.dtypes.float32\n",
    "        ).to_tensor()\n",
    "\n",
    "        # The last rating in the sequence is the target for the model to predict.\n",
    "        target = sequence_ratings[:, -1]\n",
    "        features[\"sequence_ratings\"] = sequence_ratings[:, :-1]\n",
    "\n",
    "        return features, target\n",
    "\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        field_delim=\"|\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(process)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qp4k9l41EKP6"
   },
   "source": [
    "## Create model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SeuQOHrHEKP6"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_model_inputs():\n",
    "    return {\n",
    "        \"user_id\": layers.Input(name=\"user_id\", shape=(1,), dtype=tf.string),\n",
    "        \"sequence_movie_ids\": layers.Input(\n",
    "            name=\"sequence_movie_ids\", shape=(sequence_length - 1,), dtype=tf.string\n",
    "        ),\n",
    "        \"target_movie_id\": layers.Input(\n",
    "            name=\"target_movie_id\", shape=(1,), dtype=tf.string\n",
    "        ),\n",
    "        \"sequence_ratings\": layers.Input(\n",
    "            name=\"sequence_ratings\", shape=(sequence_length - 1,), dtype=tf.float32\n",
    "        ),\n",
    "        \"sex\": layers.Input(name=\"sex\", shape=(1,), dtype=tf.string),\n",
    "        \"age_group\": layers.Input(name=\"age_group\", shape=(1,), dtype=tf.string),\n",
    "        \"occupation\": layers.Input(name=\"occupation\", shape=(1,), dtype=tf.string),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xEDXngLEKP6"
   },
   "source": [
    "## Encode input features\n",
    "\n",
    "The `encode_input_features` method works as follows:\n",
    "\n",
    "1. Each categorical user feature is encoded using `layers.Embedding`, with embedding\n",
    "dimension equals to the square root of the vocabulary size of the feature.\n",
    "The embeddings of these features are concatenated to form a single input tensor.\n",
    "\n",
    "2. Each movie in the movie sequence and the target movie is encoded `layers.Embedding`,\n",
    "where the dimension size is the square root of the number of movies.\n",
    "\n",
    "3. A multi-hot genres vector for each movie is concatenated with its embedding vector,\n",
    "and processed using a non-linear `layers.Dense` to output a vector of the same movie\n",
    "embedding dimensions.\n",
    "\n",
    "4. A positional embedding is added to each movie embedding in the sequence, and then\n",
    "multiplied by its rating from the ratings sequence.\n",
    "\n",
    "5. The target movie embedding is concatenated to the sequence movie embeddings, producing\n",
    "a tensor with the shape of `[batch size, sequence length, embedding size]`, as expected\n",
    "by the attention layer for the transformer architecture.\n",
    "\n",
    "6. The method returns a tuple of two elements:  `encoded_transformer_features` and\n",
    "`encoded_other_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8dNRCLXmEKP6"
   },
   "outputs": [],
   "source": [
    "\n",
    "def encode_input_features(\n",
    "    inputs,\n",
    "    include_user_id=True,\n",
    "    include_user_features=True,\n",
    "    include_movie_features=True,\n",
    "):\n",
    "\n",
    "    encoded_transformer_features = []\n",
    "    encoded_other_features = []\n",
    "\n",
    "    other_feature_names = []\n",
    "    if include_user_id:\n",
    "        other_feature_names.append(\"user_id\")\n",
    "    if include_user_features:\n",
    "        other_feature_names.extend(USER_FEATURES)\n",
    "\n",
    "    ## Encode user features\n",
    "    for feature_name in other_feature_names:\n",
    "        # Convert the string input values into integer indices.\n",
    "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(\n",
    "            inputs[feature_name]\n",
    "        )\n",
    "        # Compute embedding dimensions\n",
    "        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
    "        # Create an embedding layer with the specified dimensions.\n",
    "        embedding_encoder = layers.Embedding(\n",
    "            input_dim=len(vocabulary),\n",
    "            output_dim=embedding_dims,\n",
    "            name=f\"{feature_name}_embedding\",\n",
    "        )\n",
    "        # Convert the index values to embedding representations.\n",
    "        encoded_other_features.append(embedding_encoder(idx))\n",
    "\n",
    "    ## Create a single embedding vector for the user features\n",
    "    if len(encoded_other_features) > 1:\n",
    "        encoded_other_features = layers.concatenate(encoded_other_features)\n",
    "    elif len(encoded_other_features) == 1:\n",
    "        encoded_other_features = encoded_other_features[0]\n",
    "    else:\n",
    "        encoded_other_features = None\n",
    "\n",
    "    ## Create a movie embedding encoder\n",
    "    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"movie_id\"]\n",
    "    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))\n",
    "    # Create a lookup to convert string values to integer indices.\n",
    "    movie_index_lookup = StringLookup(\n",
    "        vocabulary=movie_vocabulary,\n",
    "        mask_token=None,\n",
    "        num_oov_indices=0,\n",
    "        name=\"movie_index_lookup\",\n",
    "    )\n",
    "    # Create an embedding layer with the specified dimensions.\n",
    "    movie_embedding_encoder = layers.Embedding(\n",
    "        input_dim=len(movie_vocabulary),\n",
    "        output_dim=movie_embedding_dims,\n",
    "        name=f\"movie_embedding\",\n",
    "    )\n",
    "    # Create a vector lookup for movie genres.\n",
    "    genre_vectors = movies[genres].to_numpy()\n",
    "    movie_genres_lookup = layers.Embedding(\n",
    "        input_dim=genre_vectors.shape[0],\n",
    "        output_dim=genre_vectors.shape[1],\n",
    "        embeddings_initializer=tf.keras.initializers.Constant(genre_vectors),\n",
    "        trainable=False,\n",
    "        name=\"genres_vector\",\n",
    "    )\n",
    "    # Create a processing layer for genres.\n",
    "    movie_embedding_processor = layers.Dense(\n",
    "        units=movie_embedding_dims,\n",
    "        activation=\"relu\",\n",
    "        name=\"process_movie_embedding_with_genres\",\n",
    "    )\n",
    "\n",
    "    ## Define a function to encode a given movie id.\n",
    "    def encode_movie(movie_id):\n",
    "        # Convert the string input values into integer indices.\n",
    "        movie_idx = movie_index_lookup(movie_id)\n",
    "        movie_embedding = movie_embedding_encoder(movie_idx)\n",
    "        encoded_movie = movie_embedding\n",
    "        if include_movie_features:\n",
    "            movie_genres_vector = movie_genres_lookup(movie_idx)\n",
    "            encoded_movie = movie_embedding_processor(\n",
    "                layers.concatenate([movie_embedding, movie_genres_vector])\n",
    "            )\n",
    "        return encoded_movie\n",
    "\n",
    "    ## Encoding target_movie_id\n",
    "    target_movie_id = inputs[\"target_movie_id\"]\n",
    "    encoded_target_movie = encode_movie(target_movie_id)\n",
    "\n",
    "    ## Encoding sequence movie_ids.\n",
    "    sequence_movies_ids = inputs[\"sequence_movie_ids\"]\n",
    "    encoded_sequence_movies = encode_movie(sequence_movies_ids)\n",
    "    # Create positional embedding.\n",
    "    position_embedding_encoder = layers.Embedding(\n",
    "        input_dim=sequence_length,\n",
    "        output_dim=movie_embedding_dims,\n",
    "        name=\"position_embedding\",\n",
    "    )\n",
    "    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)\n",
    "    encodded_positions = position_embedding_encoder(positions)\n",
    "    # Retrieve sequence ratings to incorporate them into the encoding of the movie.\n",
    "    sequence_ratings = tf.expand_dims(inputs[\"sequence_ratings\"], -1)\n",
    "    # Add the positional encoding to the movie encodings and multiply them by rating.\n",
    "    encoded_sequence_movies_with_poistion_and_rating = layers.Multiply()(\n",
    "        [(encoded_sequence_movies + encodded_positions), sequence_ratings]\n",
    "    )\n",
    "\n",
    "    # Construct the transformer inputs.\n",
    "    for encoded_movie in tf.unstack(\n",
    "        encoded_sequence_movies_with_poistion_and_rating, axis=1\n",
    "    ):\n",
    "        encoded_transformer_features.append(tf.expand_dims(encoded_movie, 1))\n",
    "    encoded_transformer_features.append(encoded_target_movie)\n",
    "\n",
    "    encoded_transformer_features = layers.concatenate(\n",
    "        encoded_transformer_features, axis=1\n",
    "    )\n",
    "\n",
    "    return encoded_transformer_features, encoded_other_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZUgXAijEKP6"
   },
   "source": [
    "## Create a BST model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TqPtvxcnEKP6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sujaykumar/miniconda3/envs/tensorflow/lib/python3.10/site-packages/numpy/core/numeric.py:2468: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "include_user_id = False\n",
    "include_user_features = False\n",
    "include_movie_features = False\n",
    "\n",
    "hidden_units = [256, 128]\n",
    "dropout_rate = 0.1\n",
    "num_heads = 3\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs = create_model_inputs()\n",
    "    transformer_features, other_features = encode_input_features(\n",
    "        inputs, include_user_id, include_user_features, include_movie_features\n",
    "    )\n",
    "\n",
    "    # Create a multi-headed attention layer.\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=transformer_features.shape[2], dropout=dropout_rate\n",
    "    )(transformer_features, transformer_features)\n",
    "\n",
    "    # Transformer block.\n",
    "    attention_output = layers.Dropout(dropout_rate)(attention_output)\n",
    "    x1 = layers.Add()([transformer_features, attention_output])\n",
    "    x1 = layers.LayerNormalization()(x1)\n",
    "    x2 = layers.LeakyReLU()(x1)\n",
    "    x2 = layers.Dense(units=x2.shape[-1])(x2)\n",
    "    x2 = layers.Dropout(dropout_rate)(x2)\n",
    "    transformer_features = layers.Add()([x1, x2])\n",
    "    transformer_features = layers.LayerNormalization()(transformer_features)\n",
    "    features = layers.Flatten()(transformer_features)\n",
    "\n",
    "    # Included the other features.\n",
    "    if other_features is not None:\n",
    "        features = layers.concatenate(\n",
    "            [features, layers.Reshape([other_features.shape[-1]])(other_features)]\n",
    "        )\n",
    "\n",
    "    # Fully-connected layers.\n",
    "    for num_units in hidden_units:\n",
    "        features = layers.Dense(num_units)(features)\n",
    "        features = layers.BatchNormalization()(features)\n",
    "        features = layers.LeakyReLU()(features)\n",
    "        features = layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    outputs = layers.Dense(units=1)(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lizlFz3dEKP7"
   },
   "source": [
    "## Run training and evaluation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROUNDS = 10\n",
    "NUM_CLIENTS = 10\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS_PER_ROUND = 5\n",
    "LEARNING_RATE = 0.01\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "EPSILON = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OSE-UV1WEKP7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1598/1598 [==============================] - 22s 13ms/step - loss: 1.1333 - mean_absolute_error: 0.8492\n",
      "Epoch 2/5\n",
      "1598/1598 [==============================] - 21s 13ms/step - loss: 1.0184 - mean_absolute_error: 0.8077\n",
      "Epoch 3/5\n",
      "1598/1598 [==============================] - 21s 13ms/step - loss: 1.0198 - mean_absolute_error: 0.8092\n",
      "Epoch 4/5\n",
      "1598/1598 [==============================] - 22s 14ms/step - loss: 1.0147 - mean_absolute_error: 0.8068\n",
      "Epoch 5/5\n",
      "1598/1598 [==============================] - 22s 14ms/step - loss: 1.0074 - mean_absolute_error: 0.8033\n",
      "Test MAE: 0.826\n"
     ]
    }
   ],
   "source": [
    "# Compile the model.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")\n",
    "\n",
    "# Read the training data.\n",
    "train_dataset = get_dataset_from_csv(\"train_data.csv\", shuffle=True, batch_size=265)\n",
    "\n",
    "# Fit the model with the training data.\n",
    "model.fit(train_dataset, epochs=EPOCHS_PER_ROUND)\n",
    "\n",
    "# Read the test data.\n",
    "test_dataset = get_dataset_from_csv(\"test_data.csv\", batch_size=265)\n",
    "\n",
    "# Evaluate the model on the test data.\n",
    "_, rmse = model.evaluate(test_dataset, verbose=0)\n",
    "print(f\"Test MAE: {round(rmse, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_clients = np.array_split(pd.read_csv(\"train_data.csv\"),NUM_CLIENTS)\n",
    "x_test_clients = np.array_split(pd.read_csv(\"test_data.csv\"),NUM_CLIENTS)\n",
    "if not os.path.exists(\"client_data\"):\n",
    "    os.mkdir(\"client_data\")\n",
    "    os.mkdir(\"client_data/train_clients\")\n",
    "    os.mkdir(\"client_data/test_clients\")\n",
    "for i in range(0, NUM_CLIENTS):\n",
    "    train_filename = f\"client_data/train_clients/train_{i + 1}.csv\"\n",
    "    test_filename = f\"client_data/test_clients/test_{i + 1}.csv\"\n",
    "\n",
    "    x_train_clients[i].to_csv(train_filename, index=False)\n",
    "    x_test_clients[i].to_csv(test_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERVER ADAM OPTIMIZER\n",
    "fedadam = tf.optimizers.Adam(learning_rate=LEARNING_RATE,beta_1=BETA_1,beta_2 = BETA_2,epsilon=EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sujaykumar/miniconda3/envs/tensorflow/lib/python3.10/site-packages/numpy/core/numeric.py:2468: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "global_model = create_model()\n",
    "# Compile the model.\n",
    "global_model.compile(\n",
    "    optimizer=fedadam,\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sujaykumar/miniconda3/envs/tensorflow/lib/python3.10/site-packages/numpy/core/numeric.py:2468: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "client_losses = []\n",
    "global_losses = []\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "client_mae = []\n",
    "global_maes = []\n",
    "\n",
    "client_models = []\n",
    "for i in range(NUM_CLIENTS):\n",
    "    local_model = create_model()\n",
    "    local_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        metrics=[keras.metrics.MeanAbsoluteError()],\n",
    "    )\n",
    "    train_dataset = get_dataset_from_csv(f\"client_data/train_clients/train_{i + 1}.csv\", shuffle=True, batch_size=BATCH_SIZE)\n",
    "    test_dataset = get_dataset_from_csv(f\"client_data/test_clients/test_{i + 1}.csv\", shuffle=True, batch_size=BATCH_SIZE)\n",
    "    local_model.fit(train_dataset,epochs=EPOCHS_PER_ROUND, batch_size=BATCH_SIZE,verbose=0)\n",
    "    evaluation = local_model.evaluate(test_dataset)\n",
    "    train_loss = evaluation[0]\n",
    "    test_loss = evaluation[1]\n",
    "    client_train_losses.append(train_loss)\n",
    "    client_losses.append(test_loss)\n",
    "    client_models.append(local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Federated Averaging\n",
    "weights = global_model.get_weights()\n",
    "for i in range(len(weights)):\n",
    "    for j in range(NUM_CLIENTS):\n",
    "        client_weights = client_models[j].get_weights()\n",
    "        weights[i] += client_weights[i]/NUM_CLIENTS\n",
    "\n",
    "global_model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = train_dataset = get_dataset_from_csv(\"valid_data.csv\", shuffle=True, batch_size=BATCH_SIZE)\n",
    "_,mae = global_model.evaluate(valid)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = list(range(1, NUM_CLIENTS + 1))\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot global accuracy and MAE\n",
    "ax.plot(num_rounds, g_acc, marker='o', linestyle='-', color='g', label='Global Accuracy')\n",
    "ax.plot(num_rounds, g_mae, marker='o', linestyle='-', color='y', label='Global MAE')\n",
    "ax.set_xlabel('Num Rounds')\n",
    "ax.set_ylabel('Accuracy and MAE')\n",
    "ax.grid(True)\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "# Save the combined plot\n",
    "plt.savefig(\"global_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2zeyyB3EKP7"
   },
   "source": [
    "You should achieve a Mean Absolute Error (MAE) at or around 0.7 on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FACX8EntEKP7"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The BST model uses the Transformer layer in its architecture to capture the sequential signals underlying\n",
    "users’ behavior sequences for recommendation.\n",
    "\n",
    "You can try training this model with different configurations, for example, by increasing\n",
    "the input sequence length and training the model for a larger number of epochs. In addition,\n",
    "you can try including other features like movie release year and customer\n",
    "zipcode, and including cross features like sex X genre."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "movielens_recommendations_transformers",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
